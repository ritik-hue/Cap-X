{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3nxUolmcmWlKqSCrfRvXK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ritik-hue/Cap-X/blob/main/Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqMUhcQZVtSr",
        "outputId": "6e310a35-e8c2-4fbc-93e6-f68b16643c04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praw\n",
            "  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting prawcore<3,>=2.4 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting update_checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2024.8.30)\n",
            "Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.3/189.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Installing collected packages: update_checker, prawcore, praw\n",
            "Successfully installed praw-7.8.1 prawcore-2.4.0 update_checker-0.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install praw"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vaderSentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5g4f2YCV85g",
        "outputId": "19500f5c-4041-4cff-9880-886a7f7371c6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2024.8.30)\n",
            "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/126.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import xgboost as xgb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM\n",
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "#Web Scraping\n",
        "\n",
        "reddit = praw.Reddit(\n",
        "    client_id=\"wvOPoMLwWmOLLPiA21g01g\",\n",
        "    client_secret=\"IU8WZOVsS3_3nRO6UlmR6R3yUZAeow\",\n",
        "    user_agent=\"python:web scrap3er:1.0 (by /u/pessimistic_singh12)\"\n",
        ")\n",
        "\n",
        "# Sentiment Analysis and Key Feature Extraction\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Fetching posts from Reddit\n",
        "subreddit = reddit.subreddit(\"wallstreetbets\")\n",
        "posts = subreddit.search(\"AAPL\", limit=50)\n",
        "\n",
        "# Extracting sentiment data\n",
        "social_media_data = []\n",
        "for post in posts:\n",
        "    title = post.title\n",
        "    body = post.selftext\n",
        "    post_date = pd.to_datetime(post.created_utc, unit='s').date()\n",
        "\n",
        "    title_tokens = word_tokenize(title)\n",
        "    filtered_title = [word for word in title_tokens if word.lower() not in stop_words]\n",
        "\n",
        "    body_tokens = word_tokenize(body)\n",
        "    filtered_body = [word for word in body_tokens if word.lower() not in stop_words]\n",
        "\n",
        "    combined_text = ' '.join(filtered_title) + ' ' + ' '.join(filtered_body)\n",
        "    sentiment = analyzer.polarity_scores(combined_text)\n",
        "\n",
        "    social_media_data.append({\n",
        "        'date': post_date,\n",
        "        'sentiment': sentiment['compound']\n",
        "    })\n",
        "\n",
        "social_media_df = pd.DataFrame(social_media_data)\n",
        "\n",
        "# Fetching stock data\n",
        "stock_data = yf.download(\"AAPL\", start=\"2022-01-01\", end=\"2022-12-31\")\n",
        "stock_data['Date'] = stock_data.index.date\n",
        "stock_data['Date'] = pd.to_datetime(stock_data['Date'])\n",
        "stock_data['Daily Return'] = stock_data['Adj Close'].pct_change()\n",
        "stock_data['Volatility'] = stock_data['Daily Return'].rolling(window=21).std()\n",
        "stock_data['Moving Average'] = stock_data['Adj Close'].rolling(window=21).mean()\n",
        "\n",
        "# Preparing data for models\n",
        "X_stock = stock_data[['Daily Return', 'Volatility', 'Moving Average']]\n",
        "y_stock = stock_data['Adj Close']\n",
        "\n",
        "X_train_stock, X_test_stock, y_train_stock, y_test_stock = train_test_split(\n",
        "    X_stock, y_stock, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# XGBoost Regression\n",
        "xgb_model = xgb.XGBRegressor()\n",
        "xgb_model.fit(X_train_stock, y_train_stock)\n",
        "y_pred_xgb = xgb_model.predict(X_test_stock)\n",
        "print(\"XGBoost Regression MSE:\", mean_squared_error(y_test_stock, y_pred_xgb))\n",
        "\n",
        "# LSTM Regression\n",
        "y_train_stock = y_train_stock.dropna()\n",
        "y_test_stock = y_test_stock.dropna()\n",
        "X_stock_lstm = X_train_stock.to_numpy().reshape(X_train_stock.shape[0], X_train_stock.shape[1], 1)\n",
        "X_test_lstm = X_test_stock.to_numpy().reshape(X_test_stock.shape[0], X_test_stock.shape[1], 1)\n",
        "\n",
        "lstm_model = Sequential()\n",
        "lstm_model.add(LSTM(units=50, return_sequences=False, input_shape=(X_stock_lstm.shape[1], 1)))\n",
        "lstm_model.add(Dense(1))\n",
        "lstm_model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "lstm_model.fit(X_stock_lstm, y_train_stock, epochs=10, batch_size=32)\n",
        "\n",
        "y_pred_lstm = lstm_model.predict(X_test_lstm)\n",
        "y_pred_lstm = np.nan_to_num(y_pred_lstm)\n",
        "y_test_stock = y_test_stock.iloc[:len(y_pred_lstm)]\n",
        "\n",
        "print(\"LSTM Regression MSE:\", mean_squared_error(y_test_stock, y_pred_lstm))\n",
        "# Processing sentiment data for classification (up/down/neutral)\n",
        "social_media_df['sentiment_class'] = social_media_df['sentiment'].apply(lambda x: 'positive' if x > 0 else ('negative' if x < 0 else 'neutral'))\n",
        "\n",
        "# Splitting sentiment data into train, validation, and test sets for classification\n",
        "X_sentiment = social_media_df[['sentiment']]\n",
        "y_sentiment = social_media_df['sentiment_class']\n",
        "\n",
        "X_train_sentiment, X_test_sentiment, y_train_sentiment, y_test_sentiment = train_test_split(X_sentiment, y_sentiment, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic Regression for Sentiment Classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "log_reg_model = LogisticRegression(max_iter=1000)\n",
        "log_reg_model.fit(X_train_sentiment, y_train_sentiment)\n",
        "\n",
        "y_pred_sentiment = log_reg_model.predict(X_test_sentiment)\n",
        "\n",
        "# Evaluate Logistic Regression Model for Sentiment Classification\n",
        "from sklearn.metrics import accuracy_score\n",
        "log_reg_accuracy = accuracy_score(y_test_sentiment, y_pred_sentiment)\n",
        "print(\"Logistic Regression Sentiment Classification Accuracy:\", log_reg_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPvWrWN9VymC",
        "outputId": "7607557e-f9a0-42e9-cbf8-984e91c58bf4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost Regression MSE: 27.332673203443203\n",
            "Epoch 1/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: nan\n",
            "Epoch 2/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: nan \n",
            "Epoch 3/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: nan \n",
            "Epoch 4/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: nan \n",
            "Epoch 5/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: nan  \n",
            "Epoch 6/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: nan  \n",
            "Epoch 7/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: nan \n",
            "Epoch 8/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: nan \n",
            "Epoch 9/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: nan \n",
            "Epoch 10/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: nan \n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step\n",
            "LSTM Regression MSE: 23629.089973988022\n",
            "Logistic Regression Sentiment Classification Accuracy: 0.8\n"
          ]
        }
      ]
    }
  ]
}